# modules/metrics_visualizer.py
"""
–ú–æ–¥—É–ª—å 5: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫
–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤: —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ vs –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ, –æ—Å—Ç–∞—Ç–∫–∏, learning/validation curves (—É–ø—Ä–æ—â—ë–Ω–Ω–æ).
"""
from typing import Any, Dict
import streamlit as st
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.model_selection import learning_curve, validation_curve
from sklearn.metrics import roc_auc_score
from scipy import integrate

def calculate_gini(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    –†–∞—Å—á–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –î–∂–∏–Ω–Ω–∏ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    Gini = 2 * AUC - 1
    """
    try:
        if len(np.unique(y_true)) == 2:  # –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
            auc = roc_auc_score(y_true, y_pred)
            gini = 2 * auc - 1
            return gini
        else:
            return None
    except Exception:
        return None

def visualize_metrics_interface(results: Dict[str, Any]) -> None:
    st.header("5. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫")
    try:
        model = results.get("model")
        X_train = results.get("X_train")
        X_test = results.get("X_test")
        y_train = results.get("y_train")
        y_test = results.get("y_test")
        y_pred_train = results.get("y_pred_train")
        y_pred_test = results.get("y_pred_test")
        problem_type = results.get("problem_type", "regression")

        st.subheader("–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏")
        st.write("Train metrics:")
        st.json(results.get("train_metrics"))
        st.write("Test metrics:")
        st.json(results.get("test_metrics"))

        # –†–∞—Å—á–µ—Ç –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –î–∂–∏–Ω–Ω–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        if problem_type == "classification" and len(np.unique(y_test)) == 2:
            try:
                # –î–ª—è –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
                if hasattr(model, "predict_proba"):
                    y_pred_proba = model.predict_proba(X_test)[:, 1]
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç predict_proba, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞–∫ –ø—Å–µ–≤–¥–æ-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
                    y_pred_proba = y_pred_test

                gini_coefficient = calculate_gini(y_test, y_pred_proba)

                if gini_coefficient is not None:
                    st.metric("–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –î–∂–∏–Ω–Ω–∏", f"{gini_coefficient:.4f}")

                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –ø–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—É –î–∂–∏–Ω–Ω–∏
                    if gini_coefficient < 0.45:
                        st.error("üö® **–í–ù–ò–ú–ê–ù–ò–ï: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –î–∂–∏–Ω–Ω–∏ –Ω–∏–∂–µ 0.45**")
                        st.warning("""
                        **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
                        - –ú–æ–¥–µ–ª—å –∏–º–µ–µ—Ç –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                        - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö
                        - –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å feature engineering
                        - –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫—É –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                        - –£–±–µ–¥–∏—Ç–µ—Å—å –≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
                        """)
                    elif gini_coefficient < 0.6:
                        st.warning("‚ö†Ô∏è **–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –î–∂–∏–Ω–Ω–∏ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0.45-0.6 - —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ**")
                    elif gini_coefficient < 0.75:
                        st.info("‚úÖ **–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –î–∂–∏–Ω–Ω–∏ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0.6-0.75 - —Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ**")
                    else:
                        st.success("üéâ **–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –î–∂–∏–Ω–Ω–∏ –≤—ã—à–µ 0.75 - –æ—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ!**")

                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
                    with st.expander("üìä –ö–∞–∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –î–∂–∏–Ω–Ω–∏?"):
                        st.markdown("""
                        **–®–∫–∞–ª–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –ø–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—É –î–∂–∏–Ω–Ω–∏:**
                        - **< 0.45**: –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ - –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–µ–Ω–∞–º–Ω–æ–≥–æ –ª—É—á—à–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ —É–≥–∞–¥—ã–≤–∞–Ω–∏—è
                        - **0.45-0.60**: –£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ - –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—É—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—É—é —Å–∏–ª—É
                        - **0.60-0.75**: –•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ - –º–æ–¥–µ–ª—å —Ö–æ—Ä–æ—à–æ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –∫–ª–∞—Å—Å—ã
                        - **> 0.75**: –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ - –º–æ–¥–µ–ª—å –æ—á–µ–Ω—å —Ç–æ—á–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–ª–∞—Å—Å—ã
                        
                        *–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –î–∂–∏–Ω–Ω–∏ = 2 √ó AUC - 1*
                        """)

            except Exception as e:
                st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –î–∂–∏–Ω–Ω–∏: {e}")

        # Actual vs Predicted (test)
        st.subheader("–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ vs –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ (test)")
        fig, ax = plt.subplots()

        if problem_type == "regression":
            ax.scatter(y_test, y_pred_test, alpha=0.6)
            minv = min(min(y_test), min(y_pred_test))
            maxv = max(max(y_test), max(y_pred_test))
            ax.plot([minv, maxv], [minv, maxv], "--", linewidth=2, color='red')
            ax.set_xlabel("–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è")
            ax.set_ylabel("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
            ax.set_title("–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ vs –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
        else:
            # –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ - confusion matrix-like visualization
            unique_classes = np.unique(y_test)
            colors = plt.cm.Set1(np.linspace(0, 1, len(unique_classes)))

            for i, cls in enumerate(unique_classes):
                mask = y_test == cls
                ax.scatter(y_test[mask], y_pred_test[mask], alpha=0.6,
                           color=colors[i], label=f'Class {cls}')

            ax.set_xlabel("–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Å—ã")
            ax.set_ylabel("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã")
            ax.set_title("–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ vs –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã")
            ax.legend()

        st.pyplot(fig)

        # Residuals (—Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏)
        if problem_type == "regression":
            st.subheader("–û—Å—Ç–∞—Ç–∫–∏ (Residuals) vs –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ")
            residuals = y_test - y_pred_test
            fig2, ax2 = plt.subplots()
            ax2.scatter(y_pred_test, residuals, alpha=0.6)
            ax2.axhline(0, color="red", linestyle="--", linewidth=2)
            ax2.set_xlabel("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
            ax2.set_ylabel("–û—Å—Ç–∞—Ç–∫–∏")
            ax2.set_title("–û—Å—Ç–∞—Ç–∫–∏ vs –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
            st.pyplot(fig2)

            # Distribution of residuals
            st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Å—Ç–∞—Ç–∫–æ–≤")
            fig3, ax3 = plt.subplots()
            ax3.hist(residuals, bins=50, alpha=0.7, edgecolor='black')
            ax3.axvline(0, color="red", linestyle="--", linewidth=2)
            ax3.set_xlabel("–û—Å—Ç–∞—Ç–∫–∏")
            ax3.set_ylabel("–ß–∞—Å—Ç–æ—Ç–∞")
            ax3.set_title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Å—Ç–∞—Ç–∫–æ–≤")
            st.pyplot(fig3)

        # Feature importance (if available)
        st.subheader("–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ / –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã")
        try:
            if hasattr(model, "feature_importances_"):
                importances = model.feature_importances_
                feat = results.get("features", [f"Feature_{i}" for i in range(len(importances))])
                df_imp = pd.DataFrame({"feature": feat, "importance": importances}).sort_values("importance", ascending=False).head(20)
                fig4, ax4 = plt.subplots(figsize=(10, 8))
                ax4.barh(df_imp["feature"][::-1], df_imp["importance"][::-1])
                ax4.set_xlabel("–í–∞–∂–Ω–æ—Å—Ç—å")
                ax4.set_title("–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
                plt.tight_layout()
                st.pyplot(fig4)

                # –¢–∞–±–ª–∏—Ü–∞ —Å –≤–∞–∂–Ω–æ—Å—Ç—è–º–∏
                st.write("–¢–æ–ø-10 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
                st.dataframe(df_imp.head(10).style.format({'importance': '{:.4f}'}))

            elif hasattr(model, "coef_"):
                coefs = model.coef_
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                if len(coefs.shape) > 1:
                    coefs = np.mean(np.abs(coefs), axis=0)  # –°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –ø–æ –∫–ª–∞—Å—Å–∞–º

                feat = results.get("features", [f"Feature_{i}" for i in range(len(coefs))])
                df_coef = pd.DataFrame({"feature": feat, "coef": coefs}).sort_values("coef", key=abs, ascending=False).head(20)
                fig5, ax5 = plt.subplots(figsize=(10, 8))
                ax5.barh(df_coef["feature"][::-1], df_coef["coef"][::-1])
                ax5.set_xlabel("–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç")
                ax5.set_title("–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –º–æ–¥–µ–ª–∏")
                plt.tight_layout()
                st.pyplot(fig5)

                # –¢–∞–±–ª–∏—Ü–∞ —Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞–º–∏
                st.write("–¢–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º–∏ –∞–±—Å–æ–ª—é—Ç–Ω—ã–º–∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞–º–∏:")
                st.dataframe(df_coef.head(10).style.format({'coef': '{:.4f}'}))
            else:
                st.info("–ú–æ–¥–µ–ª—å –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç feature_importances_ –∏–ª–∏ coef_.")
        except Exception as e:
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")

        # Learning curve (sampled if large)
        st.subheader("–ö—Ä–∏–≤–∞—è –æ–±—É—á–µ–Ω–∏—è (Learning Curve)")
        try:
            if X_train.shape[0] > 2000:
                X_sample = X_train.sample(2000, random_state=0)
                y_sample = y_train.loc[X_sample.index]
            else:
                X_sample = X_train
                y_sample = y_train

            # –í—ã–±–æ—Ä –º–µ—Ç—Ä–∏–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
            scoring = 'accuracy' if problem_type == 'classification' else 'r2'
            train_sizes, train_scores, test_scores = learning_curve(
                estimator=model,
                X=X_sample.fillna(0),
                y=y_sample,
                cv=5,
                n_jobs=-1,
                train_sizes=np.linspace(0.1, 1.0, 5),
                scoring=scoring
            )

            train_scores_mean = np.mean(train_scores, axis=1)
            test_scores_mean = np.mean(test_scores, axis=1)

            fig6, ax6 = plt.subplots(figsize=(10, 6))
            ax6.plot(train_sizes, train_scores_mean, "o-", label="–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞")
            ax6.plot(train_sizes, test_scores_mean, "o-", label="–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞")
            ax6.set_xlabel("–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏")
            ax6.set_ylabel("Score" if problem_type == 'regression' else "Accuracy")
            ax6.set_title("–ö—Ä–∏–≤–∞—è –æ–±—É—á–µ–Ω–∏—è")
            ax6.legend()
            ax6.grid(True, alpha=0.3)
            st.pyplot(fig6)

            # –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
            final_train_score = train_scores_mean[-1]
            final_test_score = test_scores_mean[-1]
            gap = final_train_score - final_test_score

            if gap > 0.1:
                st.warning("‚ö†Ô∏è **–í–æ–∑–º–æ–∂–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ**: –ë–æ–ª—å—à–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É train –∏ validation score")
            elif gap < -0.1:
                st.warning("‚ö†Ô∏è **–í–æ–∑–º–æ–∂–Ω–æ–µ –Ω–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ**: Validation score –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –≤—ã—à–µ train score")

        except Exception as e:
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å learning curve: {e}")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
        with st.expander("üìã –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"):
            st.write("**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:**")
            st.json(model.get_params())

            st.write("**–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö:**")
            st.write(f"- –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape}")
            st.write(f"- –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape}")
            st.write(f"- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(results.get('features', []))}")
            if problem_type == 'classification':
                st.write(f"- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(np.unique(y_train))}")

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –≤ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –º–µ—Ç—Ä–∏–∫: {e}")
        st.exception(e)